{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony Detection in English Tweets with Neural Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import demoji\n",
    "#demoji.download_codes()\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A: Binary Classfication of tweets as Ironic/Non-Ironic ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Replace emojis with :text:\n",
    "2) Replace @user\n",
    "3) Replace url\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskA_train_file = \"./data/taskA/train_emoji.txt\"\n",
    "taskA_test_file = \"./data/taskA/test_emoji.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(taskA_train_file, 'r') as f:\n",
    "    train_lines = [line.split('\\t') for line in f.readlines()][1:]\n",
    "assert len(train_lines) == 3834\n",
    "with open(taskA_test_file, 'r') as f:\n",
    "    test_lines = [line.split('\\t') for line in f.readlines()][1:]\n",
    "assert len(test_lines) == 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskA_train = pd.DataFrame(train_lines, columns = ['Tweet index', 'label', 'Tweet text'])\n",
    "taskA_test = pd.DataFrame(test_lines, columns = ['Tweet index', 'label', 'Tweet text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full cell of dataframe\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>label</th>\n",
       "      <th>Tweet text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Tweet index label  \\\n",
       "0  1           1      \n",
       "\n",
       "                                                                                               Tweet text  \n",
       "0  Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\\n  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskA_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>label</th>\n",
       "      <th>Tweet text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@Callisto1947 Can U Help?||More conservatives needed on #TSU + get paid 4 posting stuff like this!||YOU $ can go to http://t.co/JUmMWi0AyT\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Tweet index label  \\\n",
       "0  1           0      \n",
       "\n",
       "                                                                                                                                     Tweet text  \n",
       "0  @Callisto1947 Can U Help?||More conservatives needed on #TSU + get paid 4 posting stuff like this!||YOU $ can go to http://t.co/JUmMWi0AyT\\n  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskA_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'asked', 'God', 'to', 'protect', 'me', 'from', 'my', 'enemies', '..', 'shortly', 'after', 'I', 'started', 'losing', 'friends', 'ðŸ˜³', 'ðŸ’¯', 'or', '#naah']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tokenizer = TweetTokenizer()\n",
    "# test\n",
    "tweet = \"I asked God to protect me from my enemies .. shortly after I started losing friends ðŸ˜³ðŸ’¯  or #naah\"\n",
    "print(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  15756\n"
     ]
    }
   ],
   "source": [
    "# get full vocab for training data\n",
    "vocab = set(tokenizer.tokenize(' '.join(taskA_train['Tweet text'])))\n",
    "# sort vocab\n",
    "vocab = sorted(vocab)\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls: 917\n",
      "Number of hashtags: 2930\n",
      "Number of emojis: 158\n",
      "Number of usertag: 1987\n",
      "Number of alphanumeric and punctuations: 9764\n"
     ]
    }
   ],
   "source": [
    "# inspect vocabulary\n",
    "# urls, hashtag, usertag, number, punctuation\n",
    "urls = [token for token in vocab if 'http' in token]\n",
    "print(\"Number of urls: {}\".format(len(urls)))\n",
    "vocab_tmp = list(set(vocab) - set(urls))\n",
    "# hashtags\n",
    "hashtags = [token for token in vocab_tmp if '#' in token]\n",
    "print(\"Number of hashtags: {}\".format(len(hashtags)))\n",
    "vocab_tmp = list(set(vocab_tmp) - set(hashtags))\n",
    "# emojis\n",
    "emojis = demoji.findall(' '.join(vocab_tmp))\n",
    "print(\"Number of emojis: {}\".format(len(emojis)))\n",
    "#print(emojis)\n",
    "vocab_tmp = list(set(vocab_tmp) - set(emojis))\n",
    "usertag = [token for token in vocab_tmp if '@' in token]\n",
    "print(\"Number of usertag: {}\".format(len(usertag)))\n",
    "vocab_tmp = list(set(vocab_tmp) - set(usertag))\n",
    "# numbers \n",
    "alphanumeric = vocab_tmp\n",
    "print(\"Number of alphanumeric and punctuations: {}\".format(len(alphanumeric)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All urls will take the [URL] token\n",
    "All usertag will take the [USER] token\n",
    "All emojis will be translated to text surrounded by :\n",
    "    examples, ðŸ’¯ will be :hundred points:\n",
    "alphanumeric and puntuations will be left as they are\n",
    "'''\n",
    "# preprocess function that make the above adjustments to tweet text\n",
    "def preprocess(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    emojis = demoji.findall(text)\n",
    "    cleaned = []\n",
    "    for token in tokens:\n",
    "        if 'http' in token:\n",
    "            cleaned.append('[URL]')\n",
    "        elif '@' in token:\n",
    "            cleaned.append('[USER]')\n",
    "        elif token in emojis:\n",
    "            cleaned.append(':' + ''.join(emojis[token].split()) + ':')\n",
    "        else:\n",
    "            cleaned.append(token.lower())\n",
    "    return ' '.join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_train = taskA_train['Tweet text'].map(preprocess)\n",
    "cleaned_test = taskA_test['Tweet text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskA_train['Tweet text'] = cleaned_train\n",
    "taskA_test['Tweet text'] = cleaned_test\n",
    "taskA_train.to_csv('./preprocessed/taskA/train.csv', header = True, index = False)\n",
    "taskA_test.to_csv('./preprocessed/taskA/test.csv', header = True, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  11155\n"
     ]
    }
   ],
   "source": [
    "# write vocab file\n",
    "vocab = set(tokenizer.tokenize(' '.join(taskA_train['Tweet text'])))\n",
    "# sort vocab\n",
    "vocab = sorted(vocab)\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "#print(vocab)\n",
    "with open('./preprocessed/taskA/vocab.txt', 'w') as f:\n",
    "    for token in vocab:\n",
    "        f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ðŸ˜³': 'flushed face', 'ðŸ’¯': 'hundred points'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace Emojis with text #\n",
    "# test\n",
    "#tweet = \"@TargetZonePT ðŸ˜¡ no he bloody isn't I was upstairs getting changed !\"\n",
    "tweet = \"I asked God to protect me from my enemies .. shortly after I started losing friends ðŸ˜³ðŸ’¯  or #naah\"\n",
    "demoji.findall(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: Classifying Tweets as Ironic/Non-Ironic with a Neural Network ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tweet index', 'label', 'Tweet text'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reference: \n",
    "Natural Language Processing with PyTorch\n",
    "Delip Rao & Brian McMahan\n",
    "'''\n",
    "df = pd.read_csv(\"./preprocessed/taskA/train.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_split(infile, seed = 1):\n",
    "    folds = {}\n",
    "    random.seed(seed)\n",
    "    # create a dictionary partition\n",
    "    # partition['train']: list of training IDs\n",
    "    # partition['validation']: list of validation IDs\n",
    "    length = len(pd.read_csv(infile))\n",
    "    indices = list(range(length))\n",
    "    #random.shuffle(indices)\n",
    "    one_fold = length // 10\n",
    "    for i in range(1, 11):\n",
    "        curr_dict = {}\n",
    "        curr_dict['validation'] = indices[one_fold * (i - 1) : one_fold * i]\n",
    "        curr_dict['train'] = list(set(indices) - set(curr_dict['validation']))\n",
    "        folds[i] = curr_dict\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "def build_vocab(infile, fold, vocab_size = 10000):\n",
    "    global vocab\n",
    "    freq_dict = {}\n",
    "    df = pd.read_csv(infile)\n",
    "    df = df.iloc[fold['train'],:]\n",
    "    # concat all tweets into one string\n",
    "    tweets_string = ' '.join(df['Tweet text'])\n",
    "    # remove punctuation, convert to lower case\n",
    "    processed_tweets_string = re.sub(r'[^\\w\\s]', ' ', tweets_string).lower()\n",
    "    # split into tokens\n",
    "    tokens = processed_tweets_string.split()\n",
    "    # create count dictionary freq_dict\n",
    "    for token in tokens:\n",
    "        if token in freq_dict:\n",
    "            freq_dict[token] += 1\n",
    "        else:\n",
    "            freq_dict[token] = 1\n",
    "    # sort dictionary in descending freq count\n",
    "    sorted_freq = sorted(freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    # take top vocab_size - 1 vocab, accounting for [UNK] token\n",
    "    pruned_vocab = sorted_freq[:vocab_size - 1]\n",
    "    # add to vocab\n",
    "    vocab['[UNK]'] = 0\n",
    "    for i, token_tuple in enumerate(pruned_vocab):\n",
    "        vocab[token_tuple[0]] = i + 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(infile, vocab_size = 10000):\n",
    "    vectors = {}\n",
    "    labels = {}\n",
    "    df = pd.read_csv(infile)\n",
    "    for index, row in df.iterrows():\n",
    "        vec = [0] * vocab_size\n",
    "        tokens = list(set(re.sub(r'[^\\w\\s]', ' ', row['Tweet text']).lower().split()))\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                vec[vocab[token]] = 1\n",
    "            else:\n",
    "                vec[0] = 1\n",
    "        vectors[index] = vec\n",
    "        labels[index] = int(row['label'])\n",
    "    return (vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, list_IDs, labels):\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # generates one sample of data\n",
    "        ID = self.list_IDs[index]\n",
    "        X = torch.tensor(vectors[ID])\n",
    "        y = self.labels[ID]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "class IronyClassifier(nn.Module):\n",
    "    ''' A 2-layer Multilayer Perceptron for classifying tweets '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim (int): size of input vector(size of vocab)\n",
    "            hidden_dim (int): output size of the first linear layer\n",
    "            output_dim (int): output size of the second linear layer(number of classes)\n",
    "        '''\n",
    "        super(IronyClassifier, self).__init__()\n",
    "        # fully connected layer 1\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    ''' Forward propogation of the classifier '''    \n",
    "    def forward(self, x_in, apply_softmax = False):\n",
    "        '''\n",
    "        Args: \n",
    "            x_in (torch.Tensor): an input data tensor\n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for softmax activation\n",
    "                should be false if using Cross Entropy Loss\n",
    "        Returns:\n",
    "            result tensor. tensor.shape should be (batch, output_dim)\n",
    "        '''\n",
    "        intermediate_vector = F.relu(self.fc1(x_in))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim = 1)\n",
    "            \n",
    "        return prediction_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./preprocessed/taskA/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Training  hyper parameters\n",
    "    vocab_size = 10000,\n",
    "    num_epochs = 5,\n",
    "    hidden_dim = 300,\n",
    "    learning_rate = 0.01,\n",
    "    batch_size = 16,\n",
    "    seed = 1337,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross validation\n",
    "# get the indices for train and validation for each fold\n",
    "global vectors, labels\n",
    "folds = get_train_val_split(data_file)\n",
    "fold = 1\n",
    "partition = folds[fold]\n",
    "# create vocab\n",
    "build_vocab(data_file, partition, vocab_size = args.vocab_size)\n",
    "# create vectors and labels\n",
    "vectors, labels = vectorizer(data_file, vocab_size = args.vocab_size)\n",
    "# training loop\n",
    "\n",
    "training_set = Dataset(partition['train'], labels)\n",
    "training_generator = data.DataLoader(training_set, batch_size = args.batch_size, shuffle = True)\n",
    "\n",
    "validation_set = Dataset(partition['validation'], labels)\n",
    "validation_generator = data.DataLoader(validation_set, batch_size = args.batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \n",
      "  Loss: 0.6594616194014193\n",
      "  Accuracy: 60.145728114478096\n",
      "Epoch 1: \n",
      "  Loss: 0.310802695893303\n",
      "  Accuracy: 86.66351010101009\n",
      "Epoch 2: \n",
      "  Loss: 0.04322139117788606\n",
      "  Accuracy: 98.81365740740742\n",
      "Epoch 3: \n",
      "  Loss: 0.005862495930513331\n",
      "  Accuracy: 99.85532407407409\n",
      "Epoch 4: \n",
      "  Loss: 0.001837342329913048\n",
      "  Accuracy: 99.97106481481475\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "device = 'cpu'\n",
    "classifier = IronyClassifier(input_dim = args.vocab_size, hidden_dim = args.hidden_dim, output_dim = 2)\n",
    "classifier = classifier.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    # training\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier = classifier.float()\n",
    "    classifier.train()\n",
    "    batch_index = 0\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute output\n",
    "        y_pred = classifier(local_batch.float())\n",
    "        # compute loss\n",
    "        loss = loss_func(y_pred, local_labels)\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "        # produce gradients\n",
    "        loss.backward()\n",
    "        # backpropogation\n",
    "        optimizer.step()\n",
    "        # compute accuracy\n",
    "        acc_t = compute_accuracy(y_pred, local_labels)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        batch_index += 1\n",
    "    print(\"Epoch {}: \".format(epoch))\n",
    "    print(\"  Loss: {}\".format(running_loss))\n",
    "    print(\"  Accuracy: {}\".format(running_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.96875\n"
     ]
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "classifier.eval()\n",
    "batch_index = 0\n",
    "for local_batch, local_labels in validation_generator:\n",
    "    # get prediction\n",
    "    y_pred =  classifier(local_batch.float())\n",
    "    loss = loss_func(y_pred, local_labels)\n",
    "    loss_t = loss.to(\"cpu\").item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "    # compute accuracy\n",
    "    acc_t = compute_accuracy(y_pred, local_labels)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    batch_index += 1\n",
    "print(running_acc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Hyperparameter Tuning '''\n",
    "'''\n",
    "vocab size\n",
    "number of epochs\n",
    "size of hidden layer\n",
    "learning rate\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
