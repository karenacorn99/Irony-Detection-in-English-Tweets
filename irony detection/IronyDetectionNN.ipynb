{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irony Detection in English Tweets with Neural Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import demoji\n",
    "#demoji.download_codes()\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A: Binary Classfication of tweets as Ironic/Non-Ironic ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Replace emojis with :text:\n",
    "2) Replace @user\n",
    "3) Replace url\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskA_train_file = \"./data/taskA/train_emoji.txt\"\n",
    "taskA_test_file = \"./data/taskA/test_emoji.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskA_train = pd.read_csv(taskA_train_file, sep = '\\t')\n",
    "taskA_test = pd.read_csv(taskA_test_file, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full cell of dataframe\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label  \\\n",
       "0  1            1       \n",
       "\n",
       "                                                                                             Tweet text  \n",
       "0  Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskA_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@Callisto1947 Can U Help?||More conservatives needed on #TSU + get paid 4 posting stuff like this!||YOU $ can go to http://t.co/JUmMWi0AyT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label  \\\n",
       "0  1            0       \n",
       "\n",
       "                                                                                                                                   Tweet text  \n",
       "0  @Callisto1947 Can U Help?||More conservatives needed on #TSU + get paid 4 posting stuff like this!||YOU $ can go to http://t.co/JUmMWi0AyT  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskA_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'asked', 'God', 'to', 'protect', 'me', 'from', 'my', 'enemies', '..', 'shortly', 'after', 'I', 'started', 'losing', 'friends', 'ðŸ˜³', 'ðŸ’¯', 'or', '#naah']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tokenizer = TweetTokenizer()\n",
    "# test\n",
    "tweet = \"I asked God to protect me from my enemies .. shortly after I started losing friends ðŸ˜³ðŸ’¯  or #naah\"\n",
    "print(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  15772\n"
     ]
    }
   ],
   "source": [
    "# get full vocab for training data\n",
    "vocab = set(tokenizer.tokenize(' '.join(taskA_train['Tweet text'])))\n",
    "# sort vocab\n",
    "vocab = sorted(vocab)\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls: 917\n",
      "Number of hashtags: 2930\n",
      "Number of emojis: 158\n",
      "Number of usertag: 1987\n",
      "Number of alphanumeric and punctuations: 9780\n"
     ]
    }
   ],
   "source": [
    "# inspect vocabulary\n",
    "# urls, hashtag, usertag, number, punctuation\n",
    "urls = [token for token in vocab if 'http' in token]\n",
    "print(\"Number of urls: {}\".format(len(urls)))\n",
    "vocab_tmp = list(set(vocab) - set(urls))\n",
    "# hashtags\n",
    "hashtags = [token for token in vocab_tmp if '#' in token]\n",
    "print(\"Number of hashtags: {}\".format(len(hashtags)))\n",
    "vocab_tmp = list(set(vocab_tmp) - set(hashtags))\n",
    "# emojis\n",
    "emojis = demoji.findall(' '.join(vocab_tmp))\n",
    "print(\"Number of emojis: {}\".format(len(emojis)))\n",
    "#print(emojis)\n",
    "vocab_tmp = list(set(vocab_tmp) - set(emojis))\n",
    "usertag = [token for token in vocab_tmp if '@' in token]\n",
    "print(\"Number of usertag: {}\".format(len(usertag)))\n",
    "vocab_tmp = list(set(vocab_tmp) - set(usertag))\n",
    "# numbers \n",
    "alphanumeric = vocab_tmp\n",
    "print(\"Number of alphanumeric and punctuations: {}\".format(len(alphanumeric)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All urls will take the [URL] token\n",
    "All usertag will take the [USER] token\n",
    "All emojis will be translated to text surrounded by :\n",
    "    examples, ðŸ’¯ will be :hundred points:\n",
    "alphanumeric and puntuations will be left as they are\n",
    "'''\n",
    "# preprocess function that make the above adjustments to tweet text\n",
    "def preprocess(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    emojis = demoji.findall(text)\n",
    "    cleaned = []\n",
    "    for token in tokens:\n",
    "        if 'http' in token:\n",
    "            cleaned.append('[URL]')\n",
    "        elif '@' in token:\n",
    "            cleaned.append('[USER]')\n",
    "        elif token in emojis:\n",
    "            cleaned.append(':' + ''.join(emojis[token].split()) + ':')\n",
    "        else:\n",
    "            cleaned.append(token.lower())\n",
    "    return ' '.join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_train = taskA_train['Tweet text'].map(preprocess)\n",
    "cleaned_test = taskA_test['Tweet text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskA_train['Tweet text'] = cleaned_train\n",
    "taskA_test['Tweet text'] = cleaned_test\n",
    "taskA_train.to_csv('./preprocessed/taskA/train.csv', header = True, index = False)\n",
    "taskA_test.to_csv('./preprocessed/taskA/test.csv', header = True, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  11172\n"
     ]
    }
   ],
   "source": [
    "# write vocab file\n",
    "vocab = set(tokenizer.tokenize(' '.join(taskA_train['Tweet text'])))\n",
    "# sort vocab\n",
    "vocab = sorted(vocab)\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "#print(vocab)\n",
    "with open('./preprocessed/taskA/vocab.txt', 'w') as f:\n",
    "    for token in vocab:\n",
    "        f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ðŸ’¯': 'hundred points', 'ðŸ˜³': 'flushed face'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace Emojis with text #\n",
    "# test\n",
    "#tweet = \"@TargetZonePT ðŸ˜¡ no he bloody isn't I was upstairs getting changed !\"\n",
    "tweet = \"I asked God to protect me from my enemies .. shortly after I started losing friends ðŸ˜³ðŸ’¯  or #naah\"\n",
    "demoji.findall(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
